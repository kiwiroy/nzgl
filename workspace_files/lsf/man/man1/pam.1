
.ad l

.ll 72

.TH pam 1 September 2009" "" "Platform LSF Version 7.0.6"
.nh
\fBpam\fR
.sp 2
   Parallel Application Manager â€“ job starter for MPI
   applications
.sp 2

.sp 2 .SH "HP-UX vendor MPI syntax"
\fBHP-UX vendor MPI syntax\fR
.sp 2
bsub pam -mpi mpirun [mpirun_options ] mpi_app [argument ...]
.sp 2 .SH "SGI vendor MPI syntax"
\fBSGI vendor MPI syntax\fR
.sp 2
bsub pam [-n num_tasks ] -mpi -auto_place mpi_app [argument ...]
.sp 2 .SH "Generic PJL framework syntax"
\fBGeneric PJL framework syntax\fR
.sp 2
bsub pam [-t] [-v] [-n num_tasks ] -g [num_args] pjl_wrapper
[pjl_options] mpi_app [argument ...] pam [-h] pam [-V]
.sp 2 .SH "Description"
\fBDescription\fR
.sp 2
   The Parallel Application Manager (PAM) is the point of control
   for Platform LSF. PAM is fully integrated with Platform LSF to
   interface the user application with LSF. PAM acts as the
   supervisor of a parallel LSF job.
.sp 2
   MPI jobs started by pam can only be submitted through the LSF
   Batch system. PAM cannot be used interactively to start
   parallel jobs. sbatchd starts PAM on the first execution host.
.sp 2
   For all parallel application processes (tasks), PAM:
.sp 2
     o  
         Uses a vendor MPI library or an MPI Parallel Job
         Launcher (PJL); for example, mpirun, poe start a
         parallel job on a specified set of hosts in an LSF
         cluster.
.sp 2
     o  
         PAM contacts RES on each execution host allocated to the
         parallel job.
.sp 2
     o  
         PAM queries RES periodically to collect resource usage
         for each parallel task and passes control signals
         through RES to all process groups and individual running
         tasks, and cleans up tasks as needed.
.sp 2
     o  
         Passes job-level resource usage and process IDs (PIDs
         and PGIDs) to sbatchd for enforcement
.sp 2
     o  
         Collects resource usage information and exit status upon
         termination
.sp 2 .SH "Task startup for vendor MPI jobs"
\fBTask startup for vendor MPI jobs\fR
.sp 2
   The pam command starts a vendor MPI job on a specified set of
   hosts in a LSF cluster. Using pam to start an MPI job requires
   the underlying MPI system to be LSF aware, using a vendor MPI
   implementation that supports LSF (SGI IRIX vendor MPI or HP-UX
   vendor MPI).
.sp 2
   PAM uses the vendor MPI library to spawn the child processes
   needed for the parallel tasks that make up your MPI
   application. It starts these tasks on the systems allocated by
   LSF. The allocation includes the number of execution hosts
   needed, and the number of child processes needed on each host.
.sp 2 .SH "Task startup for LSF HPC generic PJL jobs"
\fBTask startup for LSF HPC generic PJL jobs\fR
.sp 2
   For parallel jobs submitted with bsub:
.sp 2
     o  
         PAM invokes the PJL, which in turn invokes the
         TaskStarter (TS).
.sp 2
     o  
         TS starts the tasks on each execution host, reports the
         process ID to PAM, and waits for the task to finish.
.sp 2
   Two environment variables allow you to run scripts or binaries
   before or after PAM is invoked. These are useful if you
   customize mpirun.lsf and have job scripts that call mpirun.lsf
   more than once.
.sp 2
     o  
         \fB$MPIRUN_LSF_PRE_EXEC\fR: Runs before PAM is invoked.
.sp 2
     o  
         \fB$MPIRUN_LSF_POST_EXEC\fR: Runs after PAM is invoked.
.sp 2 .SH "Options for vendor MPI jobs"
\fBOptions for vendor MPI jobs\fR
.sp 2
   \fB-auto_place\fR
.br
               The -auto_place option on the pam command line
               tells the SGI IRIX mpirun library to launch the
               MPI application according to the resources
               allocated by LSF.
.sp 2
   \fB-mpi\fR
.br
               In the SGI environment, the -mpi option on the
               bsub and pam command line is equivalent to the
               mpirun command.
.sp 2
               On HP-UX, you can have LSF manage the allocation
               of hosts to achieve better resource utilization by
               coordinating the start-up phase with mpirun. This
               is done by preceding the regular HP MPI mpirun
               command with:
.sp 2
               \fRbsub pam -mpi\fR
.sp 2
               For HP-UX vendor MPI jobs, the -mpi option must be
               the first option of the pam command.
.sp 2
               For example, to run a single-host job and have LSF
               select the host, the command:
.sp 2
               \fRmpirun -np 14 a.out\fR
.sp 2
               is entered as:
.sp 2
               \fRbsub pam -mpi mpirun -np 14 a.out\fR
.sp 2
   \fB-n \fInum_tasks\fB\fR
.br
               The number of processors required to run the
               parallel application, typically the same as the
               number of parallel tasks in the job. If the host
               is a multiprocessor, one host can start several
               tasks.
.sp 2
               You can use both bsub -n and pam -n in the same
               job submission. The number specified in the pam -n
               option should be less than or equal to the number
               specified by bsub -n. If the number of tasks
               specified with pam -n is greater than the number
               specified by bsub -n, the pam -n is ignored.
.sp 2
               For example, on SGI IRIX or SGI Altix, you can
               specify:
.sp 2
               \fRbsub -n 5 pam -n 2 -mpi -auto_place a.out\fR
.sp 2
               Here, the job requests 5 processors, but PAM only
               starts 2 parallel tasks.
.sp 2
   \fB\fImpi_app\fB [\fIargument\fB ...]\fR
.br
               The name of the MPI application to be run on the
               listed hosts. This must be the last argument on
               the command line.
.sp 2
   \fB-h\fR
.br
               Prints command usage to stderr and exit.
.sp 2
   \fB-V\fR
.br
               Prints LSF release version to stderr and exit.
.sp 2 .SH "Options for LSF HPC generic PJL jobs"
\fBOptions for LSF HPC generic PJL jobs\fR
.sp 2
   \fB-t\fR
.br
               This option tells pam not to print out the MPI job
               tasks summary report to the standard output. By
               default, the summary report prints out the task
               ID, the host on which it was executed, the command
               that was executed, the exit status, and the
               termination time.
.sp 2
   \fB-v\fR
.br
               Verbose mode. Displays the name of the execution
               host or hosts.
.sp 2
   \fB-g [\fInum_args\fB] \fIpjl_wrapper\fB [\fIpjl_options\fB]
   \fR
.br
               The -g option is required to use the LSF generic
               PJL framework. You must specify all the other pam
               options before -g.
.sp 2
               \fB\fR\fInum_args\fR\fB\fR
.br
                           Specifies how many space-separated
                           arguments in the command line are
                           related to the PJL (after that, the
                           remaining section of the command line
                           is assumed to be related to the binary
                           application that launches the parallel
                           tasks).
.sp 2
               \fB\fR\fIpjl_wrapper\fR\fB\fR
.br
                           The name of the PJL
.sp 2
               \fB\fR\fIpjl_options\fR\fB\fR
.br
                           Optional arguments to the PJL
.sp 2
               For example:
.sp 2
                 o  
                     A PJL named \fRno_arg_pjl\fR takes no
                     options, so \fInum_args\fR=1. The syntax is:
.sp 2
                     pam [pam_options] -g 1 no_arg_pjl job [job_options]
.sp 2
                 o  
                     A PJL is named 3_arg_pjl and takes the
                     options -a, -b, and \fIgroup_name\fR, so
                     \fInum_args\fR=4. The syntax is:
.sp 2
                     pam [pam_options] -g 4 3_arg_pjl -a -b group_name job [job_options]
.sp 2
   \fB-n \fInum_tasks\fB\fR
.br
               The number of processors required to run the MPI
               application, typically the number of parallel
               tasks in the job. If the host is a multiprocessor,
               one host can start several tasks.
.sp 2
               You can use both bsub -n and pam -n in the same
               job submission. The number specified in the pam -n
               option should be less than or equal to the number
               specified by bsub -n. If the number of tasks
               specified with pam -n is greater than the number
               specified by bsub -n, the pam -n is ignored.
.sp 2
   \fB\fImpi_app\fB [\fIargument \fB...]\fR
.br
               The name of the MPI application to be run on the
               listed hosts. This must be the last argument on
               the command line.
.sp 2
   \fB-h\fR
.br
               Prints command usage to stderr and exit.
.sp 2
   \fB-V\fR
.br
               Prints LSF release version to stderr and exit.
.sp 2 .SH "Exit Status"
\fBExit Status\fR
.sp 2
   pam exits with the exit status of mpirun or the PJL wrapper.
.sp 2 .SH "See also"
\fBSee also\fR
.sp 2
   bsub(1)
.sp 2
